# -*- coding: utf-8 -*-
"""Q/A_bKash.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uQMEdCbG52ZXHYftnvMsxUeqwvY_XGHB
"""

!pip install pymupdf transformers sentence-transformers

from google.colab import files
print("Please upload your HR PDF files.")
uploaded = files.upload()
pdf_filenames = list(uploaded.keys())
print(f"Uploaded files: {pdf_filenames}")

import fitz
import os

extracted_txt_files = []

for file_name in pdf_filenames:
    try:
        doc = fitz.open(file_name)
        text = ""
        for page in doc:
            text += page.get_text()

        txt_filename = file_name.replace(".pdf", ".txt")
        with open(txt_filename, "w", encoding="utf-8") as f:
            f.write(text)
        extracted_txt_files.append(txt_filename)
        print(f"Extracted text from {file_name} to {txt_filename}")
    except Exception as e:
        print(f"Error extracting text from {file_name}: {e}")

print("Text extraction complete.")

import re

def clean_text(text):
    text = re.sub(r'^\s*\d+\s*$', '', text, flags=re.MULTILINE)
    text = re.sub(r'\n+', '\n', text)
    text = re.sub(r'[ ]{2,}', ' ', text)
    text = '\n'.join(line.strip() for line in text.split('\n') if line.strip())
    text = text.replace('\n', ' ')
    return text.strip()

cleaned_txt_files = []

for file_name in extracted_txt_files:
    try:
        with open(file_name, "r", encoding="utf-8") as f:
            raw_text = f.read()

        cleaned_text_content = clean_text(raw_text)
        output_name = file_name.replace(".txt", "_cleaned.txt")

        with open(output_name, "w", encoding="utf-8") as f:
            f.write(cleaned_text_content)
        cleaned_txt_files.append(output_name)
        print(f"Cleaned text from {file_name} to {output_name}")
    except Exception as e:
        print(f"Error cleaning text from {file_name}: {e}")

print("Text cleaning complete.")

import json

def split_into_chunks_by_tokens(text, tokenizer, max_tokens=300, overlap=50):
    tokens = tokenizer.encode(text, add_special_tokens=False)
    chunks = []
    for i in range(0, len(tokens), max_tokens - overlap):
        chunk_tokens = tokens[i:i + max_tokens]
        chunks.append(tokenizer.decode(chunk_tokens))
    return chunks

from transformers import AutoTokenizer
MODEL_NAME = 'sentence-transformers/all-MiniLM-L6-v2'
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

chunked_data = {}
chunk_files = []

CHUNK_SIZE_TOKENS = 512
CHUNK_OVERLAP_TOKENS = 50

for file_name in cleaned_txt_files:
    try:
        with open(file_name, "r", encoding="utf-8") as f:
            text = f.read()
        chunks = split_into_chunks_by_tokens(text, tokenizer, max_tokens=CHUNK_SIZE_TOKENS, overlap=CHUNK_OVERLAP_TOKENS)
        chunked_data[file_name] = chunks

        out_file = file_name.replace("_cleaned.txt", "_chunks.json")
        with open(out_file, "w", encoding="utf-8") as f:
            json.dump(chunks, f, ensure_ascii=False, indent=2)
        chunk_files.append(out_file)
        print(f"Chunked text from {file_name} into {len(chunks)} chunks and saved to {out_file}")
    except Exception as e:
        print(f"Error chunking text from {file_name}: {e}")

print("Text chunking complete.")

import torch
from sentence_transformers import SentenceTransformer

MODEL_NAME = 'sentence-transformers/all-MiniLM-L6-v2'
model = SentenceTransformer(MODEL_NAME)

all_chunks = []
for chunks in chunked_data.values():
    all_chunks.extend(chunks)

print(f"Generating embeddings for a total of {len(all_chunks)} chunks...")

chunk_embeddings = model.encode(all_chunks, convert_to_tensor=True, show_progress_bar=True)

print(f"Generated embeddings of shape: {chunk_embeddings.shape}")

embeddings_output_file = 'all_chunks_embeddings.pt'
torch.save(chunk_embeddings, embeddings_output_file)

print(f"All chunk embeddings saved to {embeddings_output_file}")
embedding_source_map = []
current_embedding_index = 0
for filename, chunks in chunked_data.items():
    for i in range(len(chunks)):
        embedding_source_map.append({
            'source_file': filename,
            'chunk_index': i,
            'embedding_index': current_embedding_index
        })
        current_embedding_index += 1

source_map_output_file = 'embedding_source_map.json'
with open(source_map_output_file, 'w', encoding='utf-8') as f:
    json.dump(embedding_source_map, f, ensure_ascii=False, indent=2)

print(f"Embedding source map saved to {source_map_output_file}")

from google.colab import files

print("Downloading important files:")

for file in cleaned_txt_files:
    try:
        files.download(file)
        print(f"Downloaded: {file}")
    except Exception as e:
        print(f"Error downloading {file}: {e}")

for file in chunk_files:
    try:
        files.download(file)
        print(f"Downloaded: {file}")
    except Exception as e:
        print(f"Error downloading {file}: {e}")

try:
    files.download(embeddings_output_file)
    print(f"Downloaded: {embeddings_output_file}")
except Exception as e:
    print(f"Error downloading {embeddings_output_file}: {e}")

try:
    files.download(source_map_output_file)
    print(f"Downloaded: {source_map_output_file}")
except Exception as e:
    print(f"Error downloading {source_map_output_file}: {e}")

print("Download process complete.")